{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* = elementwise multiplication\n",
    "\n",
    "outer = elementwise\n",
    "\n",
    "dot = normal dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 32 50 68]\n",
      "(array([53, 58, 63]), array([[-1, -2, -3],\n",
      "       [ 0,  0,  0],\n",
      "       [ 2,  4,  6],\n",
      "       [ 4,  8, 12]]))\n"
     ]
    }
   ],
   "source": [
    "W = np.array(np.reshape([i for i in range(1,13)], (4,3)))\n",
    "x = np.array([1,2,3])\n",
    "u = np.array([-1,0,2,4])\n",
    "\n",
    "def dot(W, x):\n",
    "    value = np.dot(W, x)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_wrt_W = np.outer(u, x)  #applied to W\n",
    "        vjp_wrt_x = W.T.dot(u)  #applied to x\n",
    "        # return vjp_wrt_W, vjp_wrt_x\n",
    "        return vjp_wrt_x, vjp_wrt_W\n",
    "        \n",
    "    return value, vjp\n",
    "\n",
    "value, vjp = dot(W, x)\n",
    "print(value)\n",
    "print(vjp(u))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 1** Implement the relu function and its VJP in the format above. Using the finite difference equation (slide 13), make sure that the VJP is correct numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    value = np.maximum(0, x)\n",
    "\n",
    "    def vjp(u):\n",
    "        gdash = lambda y: 1 if y>=0 else 0\n",
    "        vjp_wrt_x = u*np.vectorize(gdash)(x)\n",
    "        return vjp_wrt_x,  \n",
    "        # The comma is important!\n",
    "    \n",
    "    return value, vjp\n",
    "\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "u = np.array([-1,0,2])\n",
    "value, vjp = relu(x)\n",
    "# print(value)\n",
    "# print(vjp(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_tanh(x):\n",
    "    value = np.tanh(x)\n",
    "\n",
    "    def vjp(u): \n",
    "        gdash = lambda z: (1/np.cosh(z))**2         #Is this even right?\n",
    "        vjp_wrt_x = u*np.vectorize(gdash)(x)\n",
    "        return vjp_wrt_x,  \n",
    "    \n",
    "    return value, vjp\n",
    "\n",
    "x = np.array([1,2,3])\n",
    "u = np.array([-1,0,2])\n",
    "value, vjp = act_tanh(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "Reusing dot and relu, implement a 2-layer MLP with a relu activation\n",
    "\n",
    "$x\\mapsto\\text{relu}(W_{1}x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73634365 1.19358144 1.11520082 1.03917841]\n",
      "(array([1.18333121, 1.02908902, 0.78316235]), array([[0.38355679, 0.39996566, 0.20778331],\n",
      "       [0.3438387 , 0.35854839, 0.18626692],\n",
      "       [0.00941022, 0.0098128 , 0.00509778],\n",
      "       [0.52819835, 0.5507951 , 0.28613964]]))\n"
     ]
    }
   ],
   "source": [
    "def mlp1(x, W1):\n",
    "    a, vjp1 = dot(W1, x)\n",
    "    b, vjp2 = relu(a)\n",
    "    \n",
    "    value = b\n",
    "    def vjp(u):\n",
    "        #Coming from the left here, so multiplying u by relu first and then that thing with the dot\n",
    "        vjp_wrt_a, = vjp2(u)\n",
    "        # vjp_wrt_W1, vjp_wrt_x = vjp1(vjp_wrt_a)\n",
    "        vjp_wrt_x, vjp_wrt_W1 = vjp1(vjp_wrt_a)\n",
    "        return vjp_wrt_x, vjp_wrt_W1\n",
    "    return value, vjp\n",
    "\n",
    "#W1: First layer weights; has shape (D, H)\n",
    "\n",
    "D = 4\n",
    "H = 3\n",
    "W1 = np.random.rand(D,H)\n",
    "x = np.random.rand(H)\n",
    "u = np.random.rand(D)\n",
    "\n",
    "val, vjp = mlp1(x, W1)\n",
    "print(val)\n",
    "print(vjp(u))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp2(x, W2, W1):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        x = input data\n",
    "        W1 = weight matrix\n",
    "        W2 = weight matrix\n",
    "    formula:\n",
    "        y = W2.q(W1.x)\n",
    "    returns:\n",
    "        value = evaluated value according to formula\n",
    "        vjp = tuple of vjp's in order d/dx, d/dW1, d/dW2\n",
    "\n",
    "    \"\"\"\n",
    "    a, vjp_dot1 = dot(W1, x)\n",
    "    b, vjp_relu = relu(a)\n",
    "    c, vjp_dot2 = dot(W2, b)\n",
    "    value = c\n",
    "\n",
    "    def vjp(u):\n",
    "        # vjp_wrt_W2, vjp_wrt_b = vjp_dot2(u)\n",
    "        vjp_wrt_b, vjp_wrt_W2 = vjp_dot2(u)\n",
    "        vjp_wrt_a, = vjp_relu(vjp_wrt_b)\n",
    "        # vjp_wrt_W1, vjp_wrt_x = vjp_dot1(vjp_wrt_a) \n",
    "        vjp_wrt_x, vjp_wrt_W1 = vjp_dot1(vjp_wrt_a) \n",
    "\n",
    "        return vjp_wrt_x, vjp_wrt_W1, vjp_wrt_W2 \n",
    "    return value, vjp\n",
    "\n",
    "\n",
    "\n",
    "def mlp3(x, W):\n",
    "    value, vjp_1 = mlp2(x, W[-2], W[-1])\n",
    "    value, vjp_2 = relu(value)\n",
    "    value, vjp_3 = dot(W[-3], value)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_wrt_Wk, vjp_wrt_x = vjp_3(u)    #order must actually be changed here\n",
    "        vjp_wrt_x, = vjp_2(vjp_wrt_x)\n",
    "        vjp_wrt_x_wrtW = vjp_1(vjp_wrt_x)\n",
    "        return vjp_wrt_x_wrtW, vjp_wrt_Wk\n",
    "\n",
    "    return value, vjp\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, H, C = [3,2,4]\n",
    "\n",
    "x = np.random.rand(H)\n",
    "W1 = np.random.rand(D,H)\n",
    "W2 = np.random.rand(C,D)\n",
    "# W3 = np.random.rand(C,C)\n",
    "# u = np.random.rand(C)\n",
    "W3 = np.random.rand(H,C)\n",
    "W4 = np.random.rand(D,H)\n",
    "W5 = np.random.rand(D,D)\n",
    "u = np.random.rand(D)\n",
    "\n",
    "# val, vjp = mlp2(x, W2, W1)\n",
    "# val, vjp = mlp3(x, [W3, W2, W1])\n",
    "# val, vjp = mlpk(x, [W5, W4, W3, W2, W1])\n",
    "\n",
    "# print(val)\n",
    "# print(vjp(u))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3** \n",
    "implement the squared loss VJP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_loss(y_pred, y):\n",
    "    residual = y_pred - y\n",
    "    \n",
    "    def vjp(u):\n",
    "        vjp_y_pred = u*(1*residual)\n",
    "        vjp_y = u*(-1*residual)\n",
    "        return vjp_y_pred, vjp_y\n",
    "\n",
    "    value = 0.5 * np.sum(residual ** 2)\n",
    "    # The code requires every output to be an array.\n",
    "    return np.array([value]), vjp\n",
    "\n",
    "y = np.random.rand(5)\n",
    "epsilon = np.random.uniform(-1, 1, 5)/5\n",
    "y_pred = y + epsilon\n",
    "u = np.array([1,2,3,4,5])\n",
    "\n",
    "val, vjp = squared_loss(y_pred, y)\n",
    "# print(val)\n",
    "# print(vjp(u))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 4**\n",
    "Implement the loss by composing mlp2 and squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, W2, W1):\n",
    "    # pred, predicted_vjp = mlp2(x, W1, W2) #Elysheva suggested a change as follows:\n",
    "    pred_value, predicted_vjp = mlp2(x, W2, W1)\n",
    "    loss_value, loss_vjp = squared_loss(pred_value, y)\n",
    "    value = loss_value\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_y, vjp_y_pred = loss_vjp(u)\n",
    "        vjp_x, vjp_W1, vjp_W2 = predicted_vjp(vjp_y_pred)\n",
    "        return vjp_x, vjp_y, vjp_W1, vjp_W2\n",
    "    \n",
    "    return value, vjp\n",
    "\n",
    "# y = np.random.rand(C)\n",
    "# u = np.random.rand(C)\n",
    "\n",
    "# val, vjp = loss(x, y, W1, W2)\n",
    "# print(val)\n",
    "# print(vjp(u))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 5** \n",
    "Implement an MLP with an arbitrary number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialiseMLP_random(inputfeatures, layers, verbose=False):\n",
    "    import random\n",
    "    dims = random.choices([i for i in range(2,11)], k=layers)\n",
    "    W = [np.random.rand(dims[0], inputfeatures)]\n",
    "    for i in range(1, len(dims)):\n",
    "        Wi = np.random.rand(dims[i], dims[i-1])\n",
    "        W.append(Wi)\n",
    "\n",
    "    W.reverse()\n",
    "    x = np.random.random(inputfeatures)\n",
    "    u = np.random.rand(dims[-1])\n",
    "\n",
    "    if verbose:\n",
    "        print(np.shape(x))\n",
    "        for i in W:\n",
    "            print(np.shape(i))\n",
    "        print(np.shape(u))\n",
    "\n",
    "    return x, W, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlpk(x, W):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x = input data\n",
    "        W = list of weight matrices ordered Wk, ..., W2, W1\n",
    "    formula:\n",
    "        Wk.q(Wk-1q(...W2q(W1x)))\n",
    "        mlp2(mlp2(mlp2(x, W2, W1), W3, W2), W4, W3)\n",
    "    returns\n",
    "        value = evaluated value of network\n",
    "        vjp = tuple of vjp's in order d/dx, d/dW1, ... , d/dWk\n",
    "    \"\"\"\n",
    "    if (len(W)>3):\n",
    "        # print(\"if\", len(W))\n",
    "        value, vjp_1 = mlpk(x, W[1:len(W)])\n",
    "    else:\n",
    "        # print(\"else\", len(W))\n",
    "        value, vjp_1 = mlp2(x, W[-2], W[-1])\n",
    "    \n",
    "    value, vjp_2 = relu(value)\n",
    "    value, vjp_3 = dot(W[0], value)\n",
    "\n",
    "    def vjp(u):\n",
    "        # vjp_wrt_Wk, vjp_wrt_x = vjp_3(u)\n",
    "        vjp_wrt_x, vjp_wrt_Wk = vjp_3(u)\n",
    "        vjp_wrt_x, = vjp_2(vjp_wrt_x)\n",
    "        vjp_wrt_x_wrtW = vjp_1(vjp_wrt_x)\n",
    "        return vjp_wrt_x_wrtW, vjp_wrt_Wk\n",
    "\n",
    "    return value, vjp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68.51259253 31.78881603 71.08672357]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x, W, u = initialiseMLP_random(4, 7, verbose=False)\n",
    "val, vjp = mlpk(x, W)\n",
    "\n",
    "print(val)\n",
    "vjp_output = list(vjp(u))\n",
    "# print(\"vjp_x\", vjp_output[0], sep='\\n')\n",
    "print(len(vjp_output[0]))\n",
    "i=0\n",
    "# for w in vjp_output[1]:\n",
    "    # print(\"W{i}\".format(i=i), w, sep='\\n')\n",
    "    # i=i+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check implementation by checking gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def check_grad_calculations(pblinreg, pblogreg, n, d):\n",
    "    from scipy.optimize import check_grad \n",
    "    print(check_grad(pblinreg.fun, pblinreg.grad, np.random.randn(d)))\n",
    "    grad_error = []\n",
    "    for i in range(n):\n",
    "        ind = np.random.choice(n,1)\n",
    "        w =  np.random.randn(d)\n",
    "        vec =  np.random.randn(d)\n",
    "        eps = pow(10.0, -7.0)\n",
    "        grad_error.append((pblinreg.f_i( ind[0], w+eps*vec) - pblinreg.f_i( ind[0], w))/eps - np.dot(pblinreg.grad_i(ind[0],w),vec)) \n",
    "    print(np.mean(grad_error))\n",
    "\n",
    "    # Check for the logistic regression problem\n",
    "    print(check_grad(pblogreg.fun, pblogreg.grad, np.random.randn(d)))\n",
    "    grad_error = []\n",
    "    for i in range(n):\n",
    "        ind = np.random.choice(n,1)\n",
    "        w =  np.random.randn(d)\n",
    "        vec =  np.random.randn(d)\n",
    "        eps = pow(10.0, -7.0)\n",
    "        grad_error.append((pblogreg.f_i( ind[0], w+eps*vec) - pblogreg.f_i( ind[0], w))/eps - np.dot(pblogreg.grad_i(ind[0],w),vec)) \n",
    "    print(np.mean(grad_error))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.24494797 0.58107635 1.54771172 1.06925227]\n",
      "(array([1.98184065, 2.69573953]), array([[0.57068706, 0.45386658],\n",
      "       [0.57159938, 0.45459214],\n",
      "       [0.60645832, 0.48231541]]), array([[0.54556394, 0.56780473, 0.26933486],\n",
      "       [0.3427808 , 0.35675481, 0.16922456],\n",
      "       [0.69600074, 0.72437433, 0.34360274],\n",
      "       [0.0898297 , 0.09349175, 0.04434727]]))\n"
     ]
    }
   ],
   "source": [
    "def mlp2(x, W1, W2):\n",
    "    a, vjp_a = mlp1(x, W1)\n",
    "    b, vjp_b = mlp1(a, W2)\n",
    "\n",
    "    def vjp(u):\n",
    "        vjp_wrt_x, vjp_wrt_W2 = vjp_b(u)\n",
    "        vjp_wrt_x, vjp_wrt_W1 = vjp_a(vjp_wrt_x)\n",
    "\n",
    "        return vjp_wrt_x, vjp_wrt_W1, vjp_wrt_W2\n",
    "    \n",
    "    return b, vjp\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "D, H, C = [3,2,4]\n",
    "x = np.random.rand(H)\n",
    "W1 = np.random.rand(D,H)\n",
    "W2 = np.random.rand(C,D)\n",
    "u = np.random.rand(C)\n",
    "\n",
    "val, vjp = mlp2(x, W1, W2)\n",
    "print(val)\n",
    "print(vjp(u))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 6**  \n",
    "Implement SGD to train your MLP on a dataset of your choice. Study the impact of depth (number of layers) and width (number of hidden units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "SGD: x_k+1 = x_k + a_k * del_f_i(x_k)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (tags/v3.9.6:db3ff76, Jun 28 2021, 15:26:21) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bfa605650d3342199cd8cd9bb236e9172d9224cad72c2719efab3fb60e30b42d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
